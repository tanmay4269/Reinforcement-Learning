{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install basic libraries\n",
    "# !pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=1)\n",
    "\n",
    "\n",
    "class Vector:\n",
    "    def __init__(self, x=0, y=0):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"({self.x}, {self.y})\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.x == other.x) and (self.y == other.y)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Vector(self.x + other.x, self.y + other.y)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Vector(self.x - other.x, self.y - other.y)\n",
    "\n",
    "    def __truediv__(self, val):\n",
    "        return Vector(self.x / val, self.y / val)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter((self.x, self.y))\n",
    "    \n",
    "    def copy(self):\n",
    "        return Vector(self.x, self.y)\n",
    "\n",
    "    def clip_update(self, new_vec, bounds):\n",
    "        # Doesn't let the position go out of bounds\n",
    "        # while retaining the component along the walls\n",
    "        \n",
    "        if new_vec.x < 0:\n",
    "            self.x = 0\n",
    "        elif new_vec.x >= bounds.x:\n",
    "            self.x = bounds.x - 1\n",
    "        else:\n",
    "            self.x = new_vec.x\n",
    "\n",
    "        if new_vec.y < 0:\n",
    "            self.y = 0\n",
    "        elif new_vec.y >= bounds.y:\n",
    "            self.y = bounds.y - 1\n",
    "        else:\n",
    "            self.y = new_vec.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Present atleast one set of evaluations for these\n",
    "        default environments. But DO NOT change the rest\n",
    "        of the code unless mentioned, changing anything\n",
    "        else might lead to disqualification.\n",
    "        \"\"\"\n",
    "        # self.environment = np.array([\n",
    "        #     ['S', 'F', 'F', 'F'],\n",
    "        #     ['F', 'O', 'F', 'O'],\n",
    "        #     ['F', 'F', 'F', 'O'],\n",
    "        #     ['O', 'F', 'F', 'G'],\n",
    "        # ])\n",
    "\n",
    "        self.environment = np.array([\n",
    "            ['S','F','F','F','O','F'],\n",
    "            ['F','F','F','F','F','F'],\n",
    "            ['F','F','O','O','F','F'],\n",
    "            ['F','F','O','O','F','F'],\n",
    "            ['F','F','O','O','F','F'],\n",
    "            ['F','F','O','O','F','F'],\n",
    "            ['F','F','F','F','F','F'],\n",
    "            ['0','F','F','F','F','G'],\n",
    "        ])\n",
    "\n",
    "        self.size = Vector(*self.environment.shape)\n",
    "        self.termination_states = []\n",
    "        \n",
    "        for y in range(self.environment.shape[0]):\n",
    "            for x in range(self.environment.shape[1]):\n",
    "                entry = self.environment[y, x]\n",
    "                pos = Vector(x, y)\n",
    "\n",
    "                if entry == 'S':\n",
    "                    self.start_pos = pos\n",
    "                elif entry == 'O':\n",
    "                    self.termination_states.append(pos)\n",
    "                elif entry == 'G':\n",
    "                    self.goal_pos = pos\n",
    "                    self.termination_states.append(pos)\n",
    "\n",
    "        self.actions = {\n",
    "            0: Vector(0, -1), # N\n",
    "            1: Vector(1, 0),  # E\n",
    "            2: Vector(-1, 0), # W\n",
    "            3: Vector(0, 1),  # S\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        Drift from original direction of control\n",
    "        to simulate noise in control dynamics.\n",
    "        \"\"\"\n",
    "        self.drift_dir_p = {\n",
    "            'front': 0.7,\n",
    "            'left': 0.125,\n",
    "            'right': 0.125,\n",
    "            'back': 0.05\n",
    "        }\n",
    "\n",
    "\n",
    "    def get_drift_dir(self, agent_action):\n",
    "        p = np.random.rand()\n",
    "        p_ = self.drift_dir_p['front']\n",
    "\n",
    "        x, y = agent_action\n",
    "        if p < p_: return agent_action\n",
    "        \n",
    "        p_ += self.drift_dir_p['left']\n",
    "        if p < p_: return Vector(-y, x)\n",
    "\n",
    "        p_ += self.drift_dir_p['right']\n",
    "        if p < p_: return Vector(y, -x)\n",
    "        \n",
    "        return Vector(-x, -y)\n",
    "\n",
    "    def get_state_transition(self, agent_pos, agent_action):\n",
    "        new_pos = agent_pos + self.get_drift_dir(agent_action)\n",
    "        agent_pos.clip_update(new_pos, bounds=self.size)\n",
    "\n",
    "        return agent_pos\n",
    "\n",
    "    def agent_terminated(self, agent):\n",
    "        return agent.get_state() in self.termination_states or agent.lifespan <= 0\n",
    "    \n",
    "    def get_reward(self, agent_pos):\n",
    "        # TODO\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, world, lifespan):\n",
    "        self.world = world\n",
    "        self.lifespan = lifespan\n",
    "\n",
    "        self.pos = Vector(0, 0)\n",
    "        self.actions = self.world.actions\n",
    "\n",
    "        # Agent's model\n",
    "        self.epsilon = 0.1  # for epsilon-greedy function, can\n",
    "                            # change this while training to\n",
    "                            # approach 0 as training completes\n",
    "                            \n",
    "        self.action_value = np.zeros(\n",
    "            (self.world.size.x, self.world.size.y, len(self.actions))\n",
    "        )\n",
    "\n",
    "    def init(self):\n",
    "        self.pos = self.world.start_pos.copy()\n",
    "\n",
    "    def get_state(self):\n",
    "        return (self.pos.x, self.pos.y)\n",
    "\n",
    "    def sample_action(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(len(self.actions))\n",
    "        else:\n",
    "            state = tuple(self.get_state())\n",
    "            action = np.argmax(self.action_value[state])\n",
    "\n",
    "        return int(action)\n",
    "\n",
    "    def get_state_action(self, action):\n",
    "        state = self.get_state()\n",
    "        return tuple(list(state) + [action])\n",
    "\n",
    "    def update_state(self, action):\n",
    "        # Returns reward\n",
    "\n",
    "        self.lifespan -= 1\n",
    "\n",
    "        self.pos = self.world.get_state_transition(self.pos, self.actions[action])\n",
    "        reward = self.world.get_reward(self.pos)\n",
    "\n",
    "        return self.pos, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        Use more config variables if you like\n",
    "        \"\"\"\n",
    "        \n",
    "        self.world = cfg['world']\n",
    "        self.agent = Agent(self.world, cfg['lifespan'])\n",
    "\n",
    "        self.max_episodes = cfg['max_episodes']\n",
    "        self.lifespan = cfg['lifespan']\n",
    "\n",
    "        self.gamma = cfg['gamma']\n",
    "        self.alpha = cfg['alpha']\n",
    "\n",
    "\n",
    "    def sarsa_train(self):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "    def q_learning_train(self):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "    def train(self, mode):\n",
    "        if mode == 'sarsa':\n",
    "            self.sarsa_train()\n",
    "        elif mode == 'q-learning':\n",
    "            self.q_learning_train()\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = GridWorld()\n",
    "\n",
    "cfg = {\n",
    "    'world': world,\n",
    "    \n",
    "    'max_episodes': 1000,\n",
    "    'lifespan': 100,\n",
    "\n",
    "    'gamma': 0.99,\n",
    "    'alpha': 0.5\n",
    "}\n",
    "\n",
    "Trainer(cfg).train(mode='sarsa')\n",
    "Trainer(cfg).train(mode='q-learning')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
