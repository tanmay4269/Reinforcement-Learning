{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Free Policy Evaluation\n",
    "## Example 1: Random Walk\n",
    "### TODO:\n",
    "- [x] Implement Monte-Carlo\n",
    "- [x] Implement Temporal Difference TD(0)\n",
    "    - [x] Plot RMS err vs Walks per ep across different `alpha` for both MC & TD(0)\n",
    "    - [x] Contrast with those in slides, idk why but they are hella different\n",
    "- [ ] Implement n-step TD \n",
    "    - [ ] Plot RMS vs `alpha` for different `n` for both online and offline setting\n",
    "    - [ ] Contrast with those in slides\n",
    "- [ ] Implement TD(`lambda`)\n",
    "    - [ ] Plot RMS vs `alpha` for different `lambda` for offline setting \n",
    "    - [ ] Contrast with those in slides\n",
    "\n",
    "Next: Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_reward(cfg, state):\n",
    "    if state == -1:\n",
    "        return 0\n",
    "    elif state == cfg['road_len']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_next_pos(cfg, curr_pos, values):\n",
    "    def rand_next(curr_pos):\n",
    "        if np.random.rand() < 0.5:\n",
    "            curr_pos -= 1\n",
    "        else:\n",
    "            curr_pos += 1\n",
    "\n",
    "        return curr_pos\n",
    "    \n",
    "    def greedy_next(curr_pos, values):\n",
    "        if curr_pos - 1 < 0:\n",
    "            curr_pos += 1\n",
    "        elif curr_pos + 1 >= cfg['road_len']: \n",
    "            curr_pos -= 1\n",
    "        else:\n",
    "            if values[curr_pos-1] < values[curr_pos+1]:\n",
    "                curr_pos += 1\n",
    "            elif values[curr_pos-1] > values[curr_pos+1]:\n",
    "                curr_pos -= 1\n",
    "            else:\n",
    "                curr_pos = rand_next(curr_pos)\n",
    "\n",
    "        return curr_pos\n",
    "    \n",
    "    if cfg['eval_policy'] == 'rand':\n",
    "        curr_pos = rand_next(curr_pos)\n",
    "    elif cfg['eval_policy'] == 'greedy':\n",
    "        curr_pos = greedy_next(curr_pos, values)\n",
    "    elif cfg['eval_policy'] == 'eps-greedy':\n",
    "        if np.random.rand() < cfg['eps-greedy/epsilon']:\n",
    "            curr_pos = rand_next(curr_pos)\n",
    "        else:\n",
    "            curr_pos = greedy_next(curr_pos, values)\n",
    "\n",
    "    return curr_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_mdp(\n",
    "        cfg,\n",
    "        optimal_policy=True,\n",
    "        optimality_threshold=1e-3):\n",
    "    \"\"\"\n",
    "    Uses random policy\n",
    "    \"\"\"\n",
    "\n",
    "    road_len = cfg['road_len']\n",
    "    gamma = cfg['gamma']\n",
    "    terminal_states = cfg['terminal_states']\n",
    "\n",
    "    values = np.full(road_len, 0.0)\n",
    "    policy = 0.5 * np.ones((road_len, 2))\n",
    "\n",
    "    for _ in range(500):\n",
    "        new_values = np.zeros_like(values)\n",
    "\n",
    "        # Value Update\n",
    "        for i in range(road_len):\n",
    "            for action, i_new in enumerate([i-1, i+1]):\n",
    "                curr_reward = get_reward(cfg, i_new)\n",
    "\n",
    "                returns = 0.0 if i_new in terminal_states else values[i_new]\n",
    "                \n",
    "                new_values[i] += policy[i, action] * (curr_reward + gamma * returns)\n",
    "\n",
    "\n",
    "        if (new_values - values < optimality_threshold).all():\n",
    "            break\n",
    "\n",
    "        values = new_values\n",
    "        if optimal_policy is False:\n",
    "            continue\n",
    "\n",
    "        # Policy Update\n",
    "        new_policy = np.zeros_like(policy)\n",
    "\n",
    "        for i in range(road_len):\n",
    "            next_values = np.zeros(2)\n",
    "            for action, i_new in enumerate([i-1, i+1]):\n",
    "                if i_new in terminal_states:\n",
    "                    next_values[action] = -np.inf\n",
    "                    continue\n",
    "                \n",
    "                new_values[action] = values[i_new]\n",
    "\n",
    "            best_actions = np.where(next_values == next_values.max())[0]\n",
    "            new_policy[i, best_actions] = 1\n",
    "            new_policy[i] /= new_policy[i].sum()\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_eval(cfg, values):\n",
    "    pos_rewards = []\n",
    "\n",
    "    # Init episode\n",
    "    if cfg['start_pos'] == 'mid':\n",
    "        curr_pos = cfg['road_len'] // 2\n",
    "\n",
    "    # Running/sampling episode\n",
    "    for step in range(cfg['max_episode_len']):\n",
    "        prev_pos = curr_pos\n",
    "        \n",
    "        curr_pos = get_next_pos(cfg, curr_pos, values)\n",
    "\n",
    "        pos_rewards.append(\n",
    "            (prev_pos, get_reward(cfg, curr_pos))\n",
    "        )\n",
    "\n",
    "        if curr_pos in cfg['terminal_states']:\n",
    "            break\n",
    "\n",
    "    # Updating values\n",
    "    new_values = values.copy()\n",
    "    returns = 0.0\n",
    "    for pos, reward in reversed(pos_rewards):\n",
    "        returns = reward + cfg['gamma'] * returns\n",
    "\n",
    "        new_values[pos] += cfg['alpha'] * (returns - new_values[pos])\n",
    "\n",
    "    return new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference_eval(cfg, values, lamda=0, n=1):\n",
    "    # Init episode\n",
    "    if cfg['start_pos'] == 'mid':\n",
    "        curr_pos = cfg['road_len'] // 2\n",
    "\n",
    "    # Running/sampling episode\n",
    "    for step in range(cfg['max_episode_len']):\n",
    "        prev_pos = curr_pos\n",
    "        \n",
    "        curr_pos = get_next_pos(cfg, curr_pos, values)\n",
    "\n",
    "        reward = get_reward(cfg, curr_pos)\n",
    "        \n",
    "        expt_reward = reward \n",
    "        if curr_pos >= 0 and curr_pos < cfg['road_len']:\n",
    "            expt_reward += cfg['gamma'] * values[curr_pos]\n",
    "\n",
    "        td_err = expt_reward - values[prev_pos]\n",
    "\n",
    "        values[prev_pos] += cfg['alpha'] * td_err\n",
    "\n",
    "        if curr_pos in cfg['terminal_states']:\n",
    "            break\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(values, gt):\n",
    "    diff = values - gt\n",
    "    return np.sqrt(np.mean(diff**2))\n",
    "\n",
    "def plot_values(values_history, gt, title=''):\n",
    "    ar = np.arange(gt.shape[0])\n",
    "\n",
    "    for episode, values in values_history:\n",
    "        plt.plot(ar, values, label=f'ep = {episode}')\n",
    "    \n",
    "    plt.plot(ar, gt, label='gt', color='black')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_rms(rms_history, title=''):\n",
    "    ar = np.arange(len(rms_history))\n",
    "\n",
    "    plt.plot(ar, rms_history, color='black')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    num_episodes = cfg['num_episodes']\n",
    "    \n",
    "    values = np.full(cfg['road_len'], 0.5)\n",
    "\n",
    "    gt = solve_mdp(cfg, optimal_policy=False)\n",
    "\n",
    "    values_history = []\n",
    "    rms_history = []\n",
    "    for episode in range(num_episodes + 1):\n",
    "        rms_history.append(rms(values, gt))\n",
    "\n",
    "        if cfg['eval_policy'] == 'eps-greedy':\n",
    "            x = (episode+1) / num_episodes\n",
    "            cfg['eps-greedy/epsilon'] = 1 - np.log(x)\n",
    "        \n",
    "        if cfg['evaluator'] == 'mc':\n",
    "            evaluator = monte_carlo_eval\n",
    "        elif cfg['evaluator'] == 'td_0':\n",
    "            evaluator = temporal_difference_eval\n",
    "\n",
    "        values = evaluator(cfg, values)\n",
    "\n",
    "        if episode % (num_episodes // 2) == 0:\n",
    "            values_history.append((episode, values.copy()))\n",
    "\n",
    "    title1 = '{} Value Function | alpha = {} | gamma = {}'.format(\n",
    "        cfg['evaluator'], cfg['alpha'], cfg['gamma'])\n",
    "    title2 = '{} RMS Plot | alpha = {} | gamma = {}'.format(\n",
    "        cfg['evaluator'], cfg['alpha'], cfg['gamma'])\n",
    "    \n",
    "    # plot_values(values_history, gt, title1)\n",
    "    # plot_rms(rms_history, title2)\n",
    "\n",
    "    return rms_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "def plot_alpha_rms_history(cfg, alpha_rms_historys, title=''):\n",
    "    x = np.arange(cfg['num_episodes'] + 1)\n",
    "    colormap = cm.get_cmap('viridis')\n",
    "\n",
    "    for alpha, rms_historys in alpha_rms_historys.items():\n",
    "        color = colormap(alpha * 5)\n",
    "        y_min = np.min(rms_historys, axis=0)\n",
    "        y_max = np.max(rms_historys, axis=0)\n",
    "        plt.fill_between(x, y_min, y_max, color=color, alpha=0.25)\n",
    "\n",
    "        mean_rms_history = np.mean(rms_historys, axis=0)\n",
    "        plt.plot(x, mean_rms_history, label=f'alpha = {alpha}', color=color)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('RMS Error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_len = 10\n",
    "\n",
    "cfg = {\n",
    "    'road_len': road_len,\n",
    "\n",
    "    'evaluator': 'td_0',  # ['mc', 'td_0', 'td_l']\n",
    "    'num_episodes': 1000,\n",
    "    'max_episode_len': 200,\n",
    "    'start_pos': 'mid',\n",
    "\n",
    "    # 'alpha': 0.01,\n",
    "    'gamma': 0.99,\n",
    "    'eval_policy': 'rand',  # ['rand', 'greedy', 'eps-greedy']    \n",
    "\n",
    "    'terminal_states': [-1, road_len]\n",
    "}\n",
    "\n",
    "if False:\n",
    "    alpha_rms_historys = defaultdict(list)\n",
    "\n",
    "    itrs = 100\n",
    "    for k in range(1, itrs+1):\n",
    "        # for alpha in [0.01, 0.05, 0.10]:  # MC\n",
    "        for alpha in [0.05, 0.1, 0.15]:  # TD\n",
    "            cfg['alpha'] = alpha\n",
    "\n",
    "            alpha_rms_historys[alpha].append(train(cfg))\n",
    "\n",
    "    title = '{}: RMS averaged across {} runs | gamma = {}'.format(\n",
    "        cfg['evaluator'].upper(),\n",
    "        itrs,\n",
    "        cfg['gamma']\n",
    "    )\n",
    "    plot_alpha_rms_history(cfg, alpha_rms_historys, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "def plot_alpha_rms(cfg, alpha_rms, title=''):\n",
    "    x = alpha_rms.keys()\n",
    "    colormap = cm.get_cmap('viridis')\n",
    "\n",
    "    y_min, y_max, y_mean = [], [], []\n",
    "    for alpha, rms in alpha_rms.items():\n",
    "        y_min.append(np.min(rms))\n",
    "        y_max.append(np.max(rms))\n",
    "        y_mean.append(np.mean(rms))\n",
    "    \n",
    "    color = 'purple'\n",
    "    plt.fill_between(x, y_min, y_max, color=color, alpha=0.5)\n",
    "    plt.plot(x, y_mean, color=color)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('RMS Error')\n",
    "    plt.xscale('log')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     plot_alpha_rms(cfg, alpha_rms, title\u001b[38;5;241m=\u001b[39mtitle)\n\u001b[1;32m     37\u001b[0m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m\n\u001b[0;32m---> 38\u001b[0m \u001b[43merr_alpha_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluator\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd_0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     41\u001b[0m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m, in \u001b[0;36merr_alpha_plot\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     24\u001b[0m         alpha \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m     25\u001b[0m         cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m alpha\n\u001b[0;32m---> 27\u001b[0m         alpha_rms[alpha]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     29\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m For \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Episodes | gamma = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     30\u001b[0m     cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluator\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[1;32m     31\u001b[0m     cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_episodes\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     32\u001b[0m     cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m plot_alpha_rms(cfg, alpha_rms, title\u001b[38;5;241m=\u001b[39mtitle)\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluator\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd_0\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     20\u001b[0m     evaluator \u001b[38;5;241m=\u001b[39m temporal_difference_eval\n\u001b[0;32m---> 22\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m (num_episodes \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     25\u001b[0m     values_history\u001b[38;5;241m.\u001b[39mappend((episode, values\u001b[38;5;241m.\u001b[39mcopy()))\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mmonte_carlo_eval\u001b[0;34m(cfg, values)\u001b[0m\n\u001b[1;32m     12\u001b[0m     curr_pos \u001b[38;5;241m=\u001b[39m get_next_pos(cfg, curr_pos, values)\n\u001b[1;32m     14\u001b[0m     pos_rewards\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     15\u001b[0m         (prev_pos, get_reward(cfg, curr_pos))\n\u001b[1;32m     16\u001b[0m     )\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_pos \u001b[38;5;129;01min\u001b[39;00m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterminal_states\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Updating values\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "road_len = 10\n",
    "\n",
    "cfg = {\n",
    "    'road_len': road_len,\n",
    "\n",
    "    'evaluator': 'mc',  # ['mc', 'td_0', 'td_l']\n",
    "    'num_episodes': 250,\n",
    "    'max_episode_len': 200,\n",
    "    'start_pos': 'mid',\n",
    "\n",
    "    # 'alpha': 0.01,\n",
    "    'gamma': 0.9,\n",
    "    'eval_policy': 'rand',  # ['rand', 'greedy', 'eps-greedy']    \n",
    "\n",
    "    'terminal_states': [-1, road_len]\n",
    "}\n",
    "\n",
    "def err_alpha_plot(cfg):\n",
    "    alpha_rms = defaultdict(list)\n",
    "\n",
    "    itrs = 100\n",
    "    for k in range(1, itrs+1):\n",
    "        for alpha in np.logspace(-3, 0, 100):\n",
    "            alpha -= 1e-3\n",
    "            cfg['alpha'] = alpha\n",
    "\n",
    "            alpha_rms[alpha].append(train(cfg)[-1])\n",
    "\n",
    "    title = '{} For {} Episodes | gamma = {}'.format(\n",
    "        cfg['evaluator'].upper(),\n",
    "        cfg['num_episodes'],\n",
    "        cfg['gamma']\n",
    "    )\n",
    "\n",
    "    plot_alpha_rms(cfg, alpha_rms, title=title)\n",
    "\n",
    "cfg['gamma'] = 0.99\n",
    "err_alpha_plot(cfg)\n",
    "\n",
    "cfg['evaluator'] = 'td_0'\n",
    "cfg['gamma'] = 0.9\n",
    "err_alpha_plot(cfg)\n",
    "\n",
    "cfg['evaluator'] = 'td_0'\n",
    "cfg['gamma'] = 0.99\n",
    "err_alpha_plot(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=1)\n",
    "\n",
    "class Vector:\n",
    "    def __init__(self, x=0, y=0):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, tuple):\n",
    "            return (self.x == other[0]) and (self.y == other[1])\n",
    "        \n",
    "        return (self.x == other.x) and (self.y == other.y)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Vector(self.x + other.x, self.y + other.y)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return Vector(self.x - other.x, self.y - other.y)\n",
    "\n",
    "    def __truediv__(self, val):\n",
    "        return Vector(self.x / val, self.y / val)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"({self.x}, {self.y})\"\n",
    "    \n",
    "    def __int__(self):\n",
    "        return (self.x, self.y)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter((self.x, self.y))\n",
    "    \n",
    "    def copy(self):\n",
    "        return Vector(self.x, self.y)\n",
    "\n",
    "    def clip_update(self, new_vec, bounds):\n",
    "        if new_vec.x < 0:\n",
    "            self.x = 0\n",
    "        elif new_vec.x >= bounds.x:\n",
    "            self.x = bounds.x - 1\n",
    "        else:\n",
    "            self.x = new_vec.x\n",
    "\n",
    "        if new_vec.y < 0:\n",
    "            self.y = 0\n",
    "        elif new_vec.y >= bounds.y:\n",
    "            self.y = bounds.y - 1\n",
    "        else:\n",
    "            self.y = new_vec.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.shape = Vector(4,4)\n",
    "        self.spawn_obstacle_p = 0.0\n",
    "\n",
    "        np.random.seed(42)  # DO NOT change this\n",
    "        self.occupancy_map = (np.random.rand(*self.shape) < self.spawn_obstacle_p).astype(int)\n",
    "        \n",
    "        self.start_pos = Vector(0,0)  # top left is (0, 0)\n",
    "        self.goal_pos = Vector(self.shape.x-1, self.shape.y-1) \n",
    "        \n",
    "        self.occupancy_map[*self.start_pos] = 0\n",
    "        self.occupancy_map[*self.goal_pos] = 1\n",
    "\n",
    "        self.actions = {\n",
    "            0: Vector(0, -1), # N\n",
    "            1: Vector(1, 0),  # E\n",
    "            2: Vector(-1, 0), # W\n",
    "            3: Vector(0, 1),  # S\n",
    "        }\n",
    "\n",
    "    def get_reward(self, agent_pos):\n",
    "        if agent_pos == self.goal_pos:\n",
    "            return 0\n",
    "\n",
    "        if self.occupancy_map[tuple(agent_pos)] == 1:\n",
    "            return -10\n",
    "\n",
    "        return -1\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, world):\n",
    "        self.world = world\n",
    "\n",
    "        self.pos = self.world.start_pos\n",
    "        self.actions = self.world.actions\n",
    "\n",
    "        self.V = np.random.rand(*self.world.shape)\n",
    "        self.V[*self.world.goal_pos] = 0\n",
    "        self.pi = np.full((*self.world.shape, len(self.actions)), 1/len(self.actions))\n",
    "\n",
    "        # self.Q = np.full((*self.world.shape, len(self.actions)), 1/len(self.actions))\n",
    "    \n",
    "    def init(self):\n",
    "        self.pos = self.world.start_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, cfg):\n",
    "        self.world = cfg['world']\n",
    "        self.agent = Agent(cfg['world'])\n",
    "        self.num_actions = len(self.agent.actions)\n",
    "\n",
    "        self.max_episodes = cfg['max_episodes']\n",
    "        self.lifespan = cfg['lifespan']\n",
    "\n",
    "        self.gamma = cfg['gamma']\n",
    "\n",
    "        self.mapping_dict = {\n",
    "            0: '^',\n",
    "            1: '>',\n",
    "            2: '<',\n",
    "            3: 'v'\n",
    "        }\n",
    "\n",
    "    def policy_iteration(self, theta=1e-3):\n",
    "        \"\"\"\n",
    "        theta: threshold for stability of self.agent.V\n",
    "        \"\"\"\n",
    "        \n",
    "        def subroutine():\n",
    "            # policy_evaluation\n",
    "            while True:\n",
    "                error = 0\n",
    "\n",
    "                # looping over all states\n",
    "                for y in range(self.world.shape.y):\n",
    "                    for x in range(self.world.shape.x):\n",
    "                        if self.world.occupancy_map[x, y] == 1:\n",
    "                            self.agent.V[x, y] = self.world.get_reward((x, y))\n",
    "                            continue\n",
    "\n",
    "                        value = self.agent.V[x, y]\n",
    "                        new_value = 0\n",
    "\n",
    "                        for pi_i in range(self.num_actions):\n",
    "                            new_pos = Vector(x, y) # placeholder name\n",
    "                            new_pos.clip_update(new_pos + self.agent.actions[pi_i], self.world.shape)\n",
    "                            \n",
    "                            reward = self.world.get_reward(new_pos)\n",
    "                            returns = self.agent.V[*new_pos]\n",
    "\n",
    "                            new_value += self.agent.pi[x, y, pi_i] * (reward + self.gamma * returns)\n",
    "\n",
    "                        error = max(error, np.abs(value - new_value))\n",
    "                        self.agent.V[x, y] = new_value\n",
    "\n",
    "                # Check exit cond for policy evaluation\n",
    "                print(f'Error = {error}')\n",
    "                if error < theta:\n",
    "                    break\n",
    "        \n",
    "            # policy_improvement\n",
    "            is_stable = True\n",
    "\n",
    "            # looping over all states\n",
    "            for y in range(self.world.shape.y):\n",
    "                for x in range(self.world.shape.x):\n",
    "                    if self.world.occupancy_map[x, y] == 1:\n",
    "                        continue\n",
    "\n",
    "                    old_action = np.argmax(self.agent.pi[x, y])\n",
    "                    new_action_values = np.zeros(self.num_actions)\n",
    "\n",
    "                    for pi_i in range(self.num_actions):\n",
    "                        new_pos = Vector(x, y) # placeholder name\n",
    "                        new_pos.clip_update(new_pos + self.agent.actions[pi_i], self.world.shape)\n",
    "\n",
    "                        reward = self.world.get_reward(new_pos)\n",
    "                        returns = self.agent.V[*new_pos]\n",
    "\n",
    "                        new_action_values[pi_i] = self.agent.pi[x, y, pi_i] * (reward + self.gamma * returns)\n",
    "\n",
    "                    best_pi = np.argmax(new_action_values)\n",
    "                    self.agent.pi[x, y] = 0\n",
    "                    self.agent.pi[x, y, best_pi] = 1\n",
    "\n",
    "\n",
    "                    if old_action != best_pi:\n",
    "                        is_stable = False\n",
    "\n",
    "            print(f'is_stable = {is_stable}')\n",
    "            # if is_stable: return\n",
    "            # subroutine()\n",
    "\n",
    "\n",
    "        for _ in range(10):\n",
    "            subroutine()\n",
    "\n",
    "        best_actions = np.argmax(self.agent.pi, axis=-1)\n",
    "        vectorized_mapping = np.vectorize(self.mapping_dict.get)\n",
    "        print(vectorized_mapping(best_actions).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error = 2.165066207096613\n",
      "Error = 1.3938365977135103\n",
      "Error = 1.132117256786799\n",
      "Error = 0.9726170421571334\n",
      "Error = 0.8475901025529984\n",
      "Error = 0.7544382336737163\n",
      "Error = 0.6597661003771726\n",
      "Error = 0.568193030627878\n",
      "Error = 0.48348012812369046\n",
      "Error = 0.40768132942586544\n",
      "Error = 0.3414490263740557\n",
      "Error = 0.28454084751736985\n",
      "Error = 0.23622807672845703\n",
      "Error = 0.195567029921472\n",
      "Error = 0.16156227056641903\n",
      "Error = 0.13325697279310234\n",
      "Error = 0.10977775450148819\n",
      "Error = 0.09035245340570164\n",
      "Error = 0.07431259439886873\n",
      "Error = 0.06108775321861337\n",
      "Error = 0.050196107986934635\n",
      "Error = 0.04123365488546327\n",
      "Error = 0.03386345555091985\n",
      "Error = 0.027805619090411682\n",
      "Error = 0.022828331742152486\n",
      "Error = 0.018740025384929027\n",
      "Error = 0.015382655758690689\n",
      "Error = 0.012626001239476992\n",
      "Error = 0.010362868066858155\n",
      "Error = 0.008505082702978228\n",
      "Error = 0.00698015728815804\n",
      "Error = 0.0057285244992559825\n",
      "Error = 0.00470125037036162\n",
      "Error = 0.0038581460546538437\n",
      "Error = 0.0031662111809236393\n",
      "Error = 0.00259835196989755\n",
      "Error = 0.0021323264856007995\n",
      "Error = 0.0017498773216715335\n",
      "Error = 0.0014360187559088189\n",
      "Error = 0.0011784510762407052\n",
      "Error = 0.0009670795247718189\n",
      "Error = 0.0007936192562087996\n",
      "Error = 0.0006512709836670183\n",
      "Error = 0.000534454696321518\n",
      "Error = 0.00043859107260679764\n",
      "Error = 0.00035992205883061956\n",
      "Error = 0.00029536360393045413\n",
      "Error = 0.0002423847923971323\n",
      "Error = 0.00019890864656169072\n",
      "Error = 0.0001632307153034418\n",
      "Error = 0.00013395226130619164\n",
      "Error = 0.00010992542988041976\n",
      "Error = 9.020825113204012e-05\n",
      "Error = 7.402771237252637e-05\n",
      "Error = 6.074945351386418e-05\n",
      "Error = 4.985289778680624e-05\n",
      "Error = 4.0910842983521434e-05\n",
      "Error = 3.357271322279587e-05\n",
      "Error = 2.755081474781207e-05\n",
      "Error = 2.260905693241e-05\n",
      "Error = 1.8553696341072623e-05\n",
      "Error = 1.5225741025304274e-05\n",
      "Error = 1.2494717186228854e-05\n",
      "Error = 1.0253553947237037e-05\n",
      "Error = 8.414385588295659e-06\n",
      "Error = 6.905106758381407e-06\n",
      "Error = 5.666545554561253e-06\n",
      "Error = 4.6501436727908185e-06\n",
      "Error = 3.816052643657031e-06\n",
      "Error = 3.1315715798285737e-06\n",
      "Error = 2.569865111823333e-06\n",
      "Error = 2.1089112998140536e-06\n",
      "Error = 1.7306382531501185e-06\n",
      "Error = 1.4202156179976555e-06\n",
      "Error = 1.1654731402188645e-06\n",
      "Error = 9.564235341485983e-07\n",
      "is_stable = False\n",
      "Error = 5.285722178509937\n",
      "Error = 4.757149960658944\n",
      "Error = 4.28143496459305\n",
      "Error = 3.853291468133736\n",
      "Error = 3.4679623213203623\n",
      "Error = 3.1211660891883257\n",
      "Error = 0\n",
      "is_stable = False\n",
      "Error = 5.217031\n",
      "Error = 0.7290000000000001\n",
      "Error = 0.6561000000000003\n",
      "Error = 0.59049\n",
      "Error = 0.531441\n",
      "Error = 0.47829690000000014\n",
      "Error = 0.43046720999999977\n",
      "Error = 0.38742048900000015\n",
      "Error = 0.3486784401000005\n",
      "Error = 0.3138105960899997\n",
      "Error = 0.28242953648099967\n",
      "Error = 0.25418658283289997\n",
      "Error = 0.2287679245496097\n",
      "Error = 0.20589113209465015\n",
      "Error = 0.18530201888518505\n",
      "Error = 0.16677181699666477\n",
      "Error = 0.1500946352969983\n",
      "Error = 0.13508517176729917\n",
      "Error = 0.12157665459056943\n",
      "Error = 0.10941898913151249\n",
      "Error = 0.0984770902183616\n",
      "Error = 0.08862938119652597\n",
      "Error = 0.0797664430768723\n",
      "Error = 0.0717897987691849\n",
      "Error = 0.06461081889226605\n",
      "Error = 0.05814973700304016\n",
      "Error = 0.052334763302736675\n",
      "Error = 0.047101286972463186\n",
      "Error = 0.042391158275215446\n",
      "Error = 0.03815204244769532\n",
      "Error = 0.034336838202925435\n",
      "Error = 0.030903154382633247\n",
      "Error = 0.027812838944369034\n",
      "Error = 0.025031555049931598\n",
      "Error = 0.022528399544938793\n",
      "Error = 0.020275559590444914\n",
      "Error = 0.018248003631400778\n",
      "Error = 0.016423203268260522\n",
      "Error = 0.014780882941435536\n",
      "Error = 0.01330279464729145\n",
      "Error = 0.011972515182561594\n",
      "Error = 0.010775263664305257\n",
      "Error = 0.009697737297875264\n",
      "Error = 0.008727963568087915\n",
      "Error = 0.007855167211278768\n",
      "Error = 0.0070696504901519575\n",
      "Error = 0.006362685441136051\n",
      "Error = 0.005726416897022801\n",
      "Error = 0.005153775207320521\n",
      "Error = 0.004638397686587581\n",
      "Error = 0.004174557917929533\n",
      "Error = 0.003757102126137113\n",
      "Error = 0.0033813919135230464\n",
      "Error = 0.003043252722170209\n",
      "Error = 0.002738927449954076\n",
      "Error = 0.002465034704957958\n",
      "Error = 0.002218531234461807\n",
      "Error = 0.0019966781110163367\n",
      "Error = 0.0017970102999136373\n",
      "Error = 0.001617309269922984\n",
      "Error = 0.0014555783429308633\n",
      "Error = 0.0013100205086367112\n",
      "Error = 0.0011790184577744611\n",
      "Error = 0.001061116611996482\n",
      "Error = 0.0009550049507964786\n",
      "Error = 0.0008595044557164755\n",
      "Error = 0.0007735540101450056\n",
      "Error = 0.000696198609130505\n",
      "Error = 0.0006265787482178098\n",
      "Error = 0.0005639208733967394\n",
      "Error = 0.0005075287860556443\n",
      "Error = 0.0004567759074500799\n",
      "Error = 0.0004110983167056048\n",
      "Error = 0.00036998848503522197\n",
      "Error = 0.00033298963653116687\n",
      "Error = 0.00029969067287893836\n",
      "Error = 0.00026972160559068925\n",
      "Error = 0.00024274944503233087\n",
      "Error = 0.00021847450052803197\n",
      "Error = 0.00019662705047629458\n",
      "Error = 0.00017696434542813222\n",
      "Error = 0.00015926791088460845\n",
      "Error = 0.00014334111979685815\n",
      "Error = 0.00012900700781770524\n",
      "Error = 0.00011610630703451363\n",
      "Error = 0.00010449567633230572\n",
      "Error = 9.404610869800933e-05\n",
      "Error = 8.464149782838604e-05\n",
      "Error = 7.617734804554743e-05\n",
      "Error = 6.855961324170323e-05\n",
      "Error = 6.170365191771054e-05\n",
      "Error = 5.553328672469604e-05\n",
      "Error = 4.9979958053114615e-05\n",
      "Error = 4.498196224744788e-05\n",
      "Error = 4.0483766023058365e-05\n",
      "Error = 3.643538942021962e-05\n",
      "Error = 3.279185047944111e-05\n",
      "Error = 2.9512665431496998e-05\n",
      "Error = 2.65613988883473e-05\n",
      "Error = 2.390525899897966e-05\n",
      "Error = 2.1514733099081695e-05\n",
      "Error = 1.9363259788462983e-05\n",
      "Error = 1.74269338106825e-05\n",
      "Error = 1.5684240429081342e-05\n",
      "Error = 1.4115816385995572e-05\n",
      "Error = 1.2704234746863108e-05\n",
      "Error = 1.1433811271999161e-05\n",
      "Error = 1.0290430145332152e-05\n",
      "Error = 9.261387130976573e-06\n",
      "Error = 8.335248418589458e-06\n",
      "Error = 7.501723576552877e-06\n",
      "Error = 6.751551218542318e-06\n",
      "Error = 6.076396097398629e-06\n",
      "Error = 5.468756487658766e-06\n",
      "Error = 4.921880838892889e-06\n",
      "Error = 4.429692754825965e-06\n",
      "Error = 3.986723479343368e-06\n",
      "Error = 3.5880511308761243e-06\n",
      "Error = 3.2292460172556048e-06\n",
      "Error = 2.9063214164182227e-06\n",
      "Error = 2.6156892740658577e-06\n",
      "Error = 2.3541203475474504e-06\n",
      "Error = 2.1187083127927053e-06\n",
      "Error = 1.906837480802892e-06\n",
      "Error = 1.7161537329002385e-06\n",
      "Error = 1.5445383603207574e-06\n",
      "Error = 1.3900845239334103e-06\n",
      "Error = 1.251076071184798e-06\n",
      "Error = 1.1259684633557754e-06\n",
      "Error = 1.0133716177307406e-06\n",
      "Error = 9.120344550694881e-07\n",
      "is_stable = False\n",
      "Error = 9.999994614527743\n",
      "Error = 8.99999515307497\n",
      "Error = 8.099995637767472\n",
      "Error = 5.983858066826997e-07\n",
      "is_stable = False\n",
      "Error = 9.999996466591654\n",
      "Error = 4.846925030221882e-07\n",
      "is_stable = False\n",
      "Error = 9.99999681993249\n",
      "Error = 8.999997137939241\n",
      "Error = 8.099997424145316\n",
      "Error = 3.1800675159843195e-07\n",
      "is_stable = False\n",
      "Error = 9.999998122201937\n",
      "Error = 2.575854693986912e-07\n",
      "is_stable = False\n",
      "Error = 9.999998309981743\n",
      "Error = 8.999998478983569\n",
      "Error = 8.099998631085212\n",
      "Error = 1.690018258670989e-07\n",
      "is_stable = False\n",
      "Error = 9.999999002061122\n",
      "Error = 1.3689147948525715e-07\n",
      "is_stable = False\n",
      "Error = 9.99999910185501\n",
      "Error = 8.99999919166951\n",
      "Error = 8.099999272502558\n",
      "Error = 8.981449894918114e-08\n",
      "is_stable = False\n",
      "[['^' '^' '^' '^']\n",
      " ['^' '^' '^' '^']\n",
      " ['^' '^' '^' '^']\n",
      " ['^' '^' '^' '^']]\n",
      "[[-10.  -10.  -10.   -1.9]\n",
      " [-10.  -10.  -10.   -1. ]\n",
      " [-10.  -10.  -10.    0. ]\n",
      " [-10.  -10.  -10.    0. ]]\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "cfg = {\n",
    "    'world': GridWorld(),\n",
    "    'max_episodes': 0,\n",
    "    'lifespan': 0,\n",
    "    'gamma': 0.9\n",
    "}\n",
    "\n",
    "trainer = Trainer(cfg)\n",
    "trainer.policy_iteration(theta=1e-6)\n",
    "\n",
    "print(trainer.agent.V)\n",
    "# print(np.argmax(trainer.agent.pi, axis=-1))\n",
    "print(trainer.world.occupancy_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
